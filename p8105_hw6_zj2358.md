p8105_hw6
================
Zhezheng Jin
2023-11-29

## Problem 1

#### Data import

``` r
homicide <- read_csv("./data_file/homicide-data.csv")

homicide
```

    ## # A tibble: 52,179 × 12
    ##    uid        reported_date victim_last  victim_first victim_race victim_age
    ##    <chr>              <dbl> <chr>        <chr>        <chr>       <chr>     
    ##  1 Alb-000001      20100504 GARCIA       JUAN         Hispanic    78        
    ##  2 Alb-000002      20100216 MONTOYA      CAMERON      Hispanic    17        
    ##  3 Alb-000003      20100601 SATTERFIELD  VIVIANA      White       15        
    ##  4 Alb-000004      20100101 MENDIOLA     CARLOS       Hispanic    32        
    ##  5 Alb-000005      20100102 MULA         VIVIAN       White       72        
    ##  6 Alb-000006      20100126 BOOK         GERALDINE    White       91        
    ##  7 Alb-000007      20100127 MALDONADO    DAVID        Hispanic    52        
    ##  8 Alb-000008      20100127 MALDONADO    CONNIE       Hispanic    52        
    ##  9 Alb-000009      20100130 MARTIN-LEYVA GUSTAVO      White       56        
    ## 10 Alb-000010      20100210 HERRERA      ISRAEL       Hispanic    43        
    ## # ℹ 52,169 more rows
    ## # ℹ 6 more variables: victim_sex <chr>, city <chr>, state <chr>, lat <dbl>,
    ## #   lon <dbl>, disposition <chr>

#### Data wrangling

``` r
homicide_clean <- homicide %>%
  # Create city_state variable
  mutate(city_state = paste(city, state, sep = ", ")) %>%
  # Omit specified cities
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) %>%
  # Limit analysis to white or black victim race
  filter(victim_race %in% c("White", "Black")) %>%
  # Create is_solved binary variable
  mutate(is_solved = if_else(disposition == "Closed by arrest", 1, 0)) %>%
  # Ensure victim_age is numeric
  mutate(victim_age = as.numeric(victim_age))

homicide_clean
```

    ## # A tibble: 39,693 × 14
    ##    uid        reported_date victim_last  victim_first victim_race victim_age
    ##    <chr>              <dbl> <chr>        <chr>        <chr>            <dbl>
    ##  1 Alb-000003      20100601 SATTERFIELD  VIVIANA      White               15
    ##  2 Alb-000005      20100102 MULA         VIVIAN       White               72
    ##  3 Alb-000006      20100126 BOOK         GERALDINE    White               91
    ##  4 Alb-000009      20100130 MARTIN-LEYVA GUSTAVO      White               56
    ##  5 Alb-000012      20100218 LUJAN        KEVIN        White               NA
    ##  6 Alb-000016      20100308 GRAY         STEFANIA     White               43
    ##  7 Alb-000018      20100323 DAVID        LARRY        White               52
    ##  8 Alb-000019      20100402 BRITO        ELIZABETH    White               22
    ##  9 Alb-000021      20100423 KING         TEVION       Black               15
    ## 10 Alb-000022      20100423 BOYKIN       CEDRIC       Black               25
    ## # ℹ 39,683 more rows
    ## # ℹ 8 more variables: victim_sex <chr>, city <chr>, state <chr>, lat <dbl>,
    ## #   lon <dbl>, disposition <chr>, city_state <chr>, is_solved <dbl>

#### Logistic regression analysis for Baltimore

``` r
# Step 1: Filter data for Baltimore, MD
baltimore_data <- homicide_clean %>%
  filter(city_state == "Baltimore, MD")

# Step 2: Prepare data (ensure factors and binary outcome)
baltimore_data <- baltimore_data %>%
  mutate(victim_sex = as.factor(victim_sex),
         victim_race = as.factor(victim_race),
         is_solved = factor(is_solved, levels = c(0, 1)))

# Step 3: Fit logistic regression model
model <- glm(is_solved ~ victim_age + victim_sex + victim_race, 
             data = baltimore_data, family = binomial())

# Step 4: Apply broom::tidy to the model object
model_tidy <- tidy(model)

# Displaying the tidy model summary
print(model_tidy)
```

    ## # A tibble: 4 × 5
    ##   term             estimate std.error statistic  p.value
    ##   <chr>               <dbl>     <dbl>     <dbl>    <dbl>
    ## 1 (Intercept)       0.310     0.171        1.81 7.04e- 2
    ## 2 victim_age       -0.00673   0.00332     -2.02 4.30e- 2
    ## 3 victim_sexMale   -0.854     0.138       -6.18 6.26e-10
    ## 4 victim_raceWhite  0.842     0.175        4.82 1.45e- 6

``` r
# Step 5: Calculate adjusted odds ratios 
adjusted_or <- exp(coef(model)["victim_sexMale"])  # For Male vs Female comparison
CI <- confint(model)  # Default confidence interval
```

    ## Waiting for profiling to be done...

``` r
CI_adjusted_or <- exp(CI["victim_sexMale", ])

# Displaying the adjusted odds ratio and its confidence interval
adjusted_or
```

    ## victim_sexMale 
    ##      0.4255117

``` r
CI_adjusted_or
```

    ##     2.5 %    97.5 % 
    ## 0.3241908 0.5575508

The adjusted odds ratio of 0.426 suggests that, in Baltimore, MD, the
odds of solving a homicide for male victims are 0.426 times the the odds
of solving a homicide for female victims, when controlling for other
factors. We are 95% confident that this true odds ratio lies between
0.324 and 0.558. The confidence interval reinforces this conclusion and
indicates a statistically significant difference, as it does not include
1.

#### GLM analysis for each city

``` r
# Step 1 & 2: Group by city and nest data
nested_data <- homicide_clean %>%
  group_by(city_state) %>%
  nest()

# Step 3: Fit logistic regression model and tidy with confidence intervals for each city
nested_data <- nested_data %>%
  mutate(model = map(data, ~glm(is_solved ~ victim_age + victim_sex + victim_race, 
                                data = .x, family = binomial())),
         tidied = map(model, ~tidy(.x, conf.int = TRUE)))

# Step 4: Extract coefficients for `victim_sexMale`
nested_data <- nested_data %>%
  mutate(ORs = map(tidied, ~filter(.x, term == "victim_sexMale") %>%
                     mutate(OR = exp(estimate),
                            CI_lower = exp(conf.low),
                            CI_upper = exp(conf.high))))

# Step 5: Unnest and organize results
GLM_results <- nested_data %>%
  select(city_state, ORs) %>%
  unnest(ORs) %>%
  select(city_state, OR, CI_lower, CI_upper)

GLM_results
```

    ## # A tibble: 47 × 4
    ## # Groups:   city_state [47]
    ##    city_state         OR CI_lower CI_upper
    ##    <chr>           <dbl>    <dbl>    <dbl>
    ##  1 Albuquerque, NM 1.77     0.825    3.76 
    ##  2 Atlanta, GA     1.00     0.680    1.46 
    ##  3 Baltimore, MD   0.426    0.324    0.558
    ##  4 Baton Rouge, LA 0.381    0.204    0.684
    ##  5 Birmingham, AL  0.870    0.571    1.31 
    ##  6 Boston, MA      0.674    0.353    1.28 
    ##  7 Buffalo, NY     0.521    0.288    0.936
    ##  8 Charlotte, NC   0.884    0.551    1.39 
    ##  9 Chicago, IL     0.410    0.336    0.501
    ## 10 Cincinnati, OH  0.400    0.231    0.667
    ## # ℹ 37 more rows

#### Plot: GLM analysis for each city

``` r
GLM_results <- GLM_results %>%
  arrange(OR)

GLM_plot <- ggplot(GLM_results, aes(x = reorder(city_state, OR), y = OR)) +
  geom_point() +  
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), width = 0.2) +  
  coord_flip() +
  labs(x = "Cities",
       y = "Adjusted ORs with CIs") +
  theme_minimal()

GLM_plot
```

<img src="p8105_hw6_zj2358_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

A noticeable trend in this plot is that most cities have ORs less than
1, suggesting that across these cities, homicides with male victims are
generally less likely to be solved than those with female victims.
Moreover, the CIs for the majority of cities cross the OR of 1,
indicating that for many cities, the difference in the likelihood of
solving homicides with male versus female victims is not statistically
significant. Some cities have wide CIs, reflecting greater uncertainty
in their estimates, while others have narrower CIs, indicating more
precise estimates.

## Problem 2

#### Weather data

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

    ## using cached file: C:\Users\11350\AppData\Local/R/cache/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2023-09-28 10:25:36.441147 (8.541)

    ## file min/max dates: 1869-01-01 / 2023-09-30

#### Bootstrap analysis for log_betas and r_squared

``` r
fit0 <- lm(tmax ~ tmin + prcp, data = weather_df)
set.seed(123)
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin + prcp, data = .x) ),
    results = map(models, tidy)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) 

log_betas <-  
  bootstrap_results %>%
  group_by(strap_number) %>%
  summarise(log_betas = log(estimate[2] * estimate[3])) %>%
  select(log_betas, strap_number)

bootstrap_results2 <- 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin + prcp, data = .x) ),
    results = map(models, glance)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) 

r_squared <- 
  bootstrap_results2 %>%
  select(r.squared, strap_number)
```

#### Fitting density plots of two estimates

``` r
ggplot(r_squared, aes(x = r.squared)) + 
  geom_density() +
  labs(title = "Distribution of R-squared") +
  theme_minimal()
```

<img src="p8105_hw6_zj2358_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

``` r
ggplot(log_betas, aes(x = log_betas)) + 
  geom_density() +
  labs(title = "Distribution of log(Beta1 * Beta2)") +
  theme_minimal()
```

<img src="p8105_hw6_zj2358_files/figure-gfm/unnamed-chunk-8-2.png" width="90%" />

``` r
r_squared_sd <-
  r_squared %>%
  summarise(r_squared_sd = sd(r.squared)) %>%
  pull(r_squared_sd)

r_squared_mean <-
  r_squared %>%
  summarise(r_squared_mean = mean(r.squared)) %>%
  pull(r_squared_mean)

log_betas_sd <- 
  log_betas %>%
  summarise(log_betas_sd = sd(as.numeric(log_betas),na.rm = TRUE)) %>%
  pull(log_betas_sd)

log_betas_mean <- 
  log_betas %>% 
  summarise(log_betas_mean = mean(as.numeric(log_betas), na.rm = TRUE)) %>%
  pull(log_betas_mean)
```

#### Confidence intervals for log(β<sup>1∗β</sup>2)

``` r
CI_result <-
  log_betas %>%
  summarize(ci_lower = quantile(log_betas, 0.025, na.rm = TRUE),
            ci_upper = quantile(log_betas, 0.975, na.rm = TRUE))

CI_result_lower <- CI_result %>% pull(ci_lower)
CI_result_upper <- CI_result %>% pull(ci_upper)
```

#### Confidence intervals for r^2

``` r
CI_result2 <-
  r_squared %>%
  summarize(ci_lower = quantile(r.squared, 0.025),
            ci_upper = quantile(r.squared, 0.975)) 

CI_result_lower2 <- CI_result2 %>% pull(ci_lower)
CI_result_upper2 <- CI_result2 %>% pull(ci_upper)
```
